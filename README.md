# KNN
A implemention of KNN
# #### KNN算法的工作原理

&emsp;存在一个样本数据集合（训练集），并且样本中的每个数据都存在标签。即我们直到样本数据集中每一样本和它所对应的标签。输入没有标签的数据以后，将新数据的每个特征与样本集中的对应的特征进行比较。然后算法提取给定新样本与样本集中特征最相似的数据的分类标签---->也就是最相近的分类标签。
&emsp;一般来说，我们只选择与新给定的样本特征距离最相近的K个数据。这就是K-近邻算法中K的出处。通常K是不大于20的整数，最后选择K个最相近的数据中出现次数最多的标签，最为新数据的分类。

&emsp;#### KNN算法的工作流程:

&emsp;对未知类别属性的数据进行以下操作:
1. 遍历数据集，计算已知类别的数据集中的点与当前点(新数据点)之间的距离;
2. 按照数据递增的次序进行排序
3. 选取与当前点距离最小的K个点
4. 确定前K个点所在类别出现的概率
5. 返回前K个点中出现概率最高的类别作为当前点的预测类别

&emsp;#### 算法需要注意的地方:
1. 归一化
&emsp;用欧式距离计算每个属性之间的距离之前，需要对所有属性进行归一化。避免数字取值大-->数字差值大的属性对距离的计算结果产生较大的影响。也就是避免某一个属性因为本身的取值范围大而导致它的影响大。归一化的是为了在计算距离时每个属性的取值范围在归一化的区间内。
2. 归一化的方法
&emsp;x'=(x-min)/(max-min)  将数据归一化到 0---1之间，包括0和1.

&emsp; #### K近邻算法的优缺点。
1. k-近邻算法是基于实例学习的。使用算法时我们必须有接近实际数据的训练样本数据。------这一点非常重要。
2. k-近邻算法必须保存全部的数据集，如果用于训练的数据集很大，必须使用大量的存储空间。
3. k-近邻必须对数据集中的每一个数据的每一个属性计算距离的值。实际使用的时候会非常耗时。
4. 另一个比较重要的缺点是------>它无法给出任何数据的基础结构信息，比如说均值，方差等等。因此我们也无法知晓平均实例样本和典型实例样本具有什么样子的特征。相反，概率测量的方法就可以解决这个问题。